---
layout: default
nav_active: index
title: CORAL Project
description: CORAL Project.
lang: en
page_name: index
---

<main class="uk-section uk-section-default">
  <div class="uk-container">
    <div class="uk-container uk-margin-small">
      <h1 class="uk-margin-remove-top">
        Constrained Retrieval-Augmented Language Model
      </h1>
      <div>
        <ul class="uk-list">
          <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#goals">Project Goals</a></li>
          <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#events">Events</a></li>
          <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#publications">Publications</a></li>
          <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#awards">Awards</a></li>  
          <li><span data-uk-icon="chevron-down"></span><a class="uk-margin-small-right" href="#partners">Partners</a></li>  
        </ul>
      </div>
    </div>
    
    <div class="uk-container uk-margin-medium">
      
      <h2 id="goals">Project Goals</h2>
      <p>
        The CORAL project aims to research methods for the construction and use of large language models (LLMs) that are subject to legal, technical, and qualitative constraints. With the fulfillment of legal requirements for the training data of LLMs and the referential provenance of the generated texts, our focus is on two central criteria that are indispensable for the professional use of language models. To this end, we are researching new methods for the constrained training of LLMs and the retrieval augmented generation (RAG) of texts.
      </p>
      
      <!-- <p>Die Nutzung von Sprachmodellen unterliegt häufig einschränkenden Anforderungen, sogenannten Constraints. Wenn zum Beispiel lizenzrechtlich beschränkte Daten für das Training verwendet werden, sollen diese regelmäßig nicht in künstlich generierten Texten reproduziert werden. Aussagen in generierten Texten sollen darüber hinaus anhand von Quellen transparent nachvollziehbar sein. Solche und ähnliche Anforderungen sind für viele Institutionen und Unternehmen unverzichtbar für den produktiven und sicheren Einsatz von Sprachmodellen. Die Frage ist also, ob und wie Constraints bei der Konstruktion von Sprachmodellen berücksichtigt werden können. Genau hier setzt das Forschungsvorhaben „CORAL“ an und will Künstliche Intelligenz flexibler, resilienter und effizienter gestalten.</p> -->
      
      <div class="uk-grid-small uk-child-width-1-2@m uk-child-width-1-1@s" uk-grid>
        <img class="uk-preserve-width" src="{{ '/img/coral-illustration-small.png' | relative_url }}" alt="Coral Illustration"/>
        <div>
          <h3>Data</h3>
          <!-- <p>Das Projekt untersucht, ob praktisch nutzbare Sprachmodelle auf Basis von Texten trainiert werden können, die nur in verschiedenen eingeschränkten Formen zur Verfügung gestellt werden dürfen. Außerdem werden Methoden entwickelt, um Texte unter Berücksichtigung von Fachwissen mit Quellenangaben zu generieren. Insbesondere soll die Textreproduktion aus den Trainingsdaten vermieden, jedoch vorgegebene Quellen akkurat wiedergeben, werden. Diese Verfahren werden in aufwendigen Experimenten evaluiert und mit Partnern aus dem Finanzwesen, GLAM-Institutionen (Kultur- und Gedächtnisinstitutionen) und der Privatwirtschaft getestet.</p> -->
          <p>
            CORAL uses data from the partners involved. This includes the digital holdings of the German National Library (DNB), and web crawls from the Internet Archive and the Common Crawl, amounting to petabytes, many years of European-language news crawls from Wortschatz Leipzig, and proprietary data from the financial sector. Apart from the Common Crawl, this data has so far not been usable for training LLMs, as it is not made publicly available in its original form for legal reasons. We are therefore investigating the extent to which this data can be used legally for the training of LLMs in obfuscated form and how far the obfuscation of the data may go in order to construct useful large language models.
          </p>

          <h3 class="uk-margin-small">Approach</h3>
          <p>
            The project investigates whether practically usable language models 
            can be trained on texts that are only available in various restricted forms.
            In addition, we develop methods to generate texts that incorporate domain knowledge 
            and provide source references.
            In particular, the reproduction of training data should be avoided, 
            while accurately referencing the specified sources.
            These methods will be evaluated through extensive experiments and tested 
            in collaboration with partners from the financial sector, GLAM institutions 
            (galleries, libraries, archives, and museums), and private industry.
          </p>
          
          
        </div>
      </div>

      <h3 class="uk-margin-small">Innovations and Perspectives</h3>
      <p>
      Innovative results and insights are expected in three key areas of language model development 
      and use across society, science, and industry: (1)&nbsp;consideration of previously restricted training data;
      (2)&nbsp;model architectures that incorporate constraints, such as preventing the reproduction of training data; and
      (3)&nbsp;referencing relevant and reliable sources on which the generated text is based.
      The exemplary transfer of these approaches will clearly demonstrate both their flexibility and effectiveness.
      </p>

      <h3 class="uk-margin-small">Research Questions</h3>
      <p>
        Central research questions are: Which training methods and model architectures are robust against data constraints? How resource-efficient can useful large language models be trained? Which methods of obfuscation, un-learning and negated augmentation effectively prevent the disclosure of protected data? How can the transparency and soundness, originality, and referenceability of the generated texts be ensured? How vulnerable are the methods used to secure the training data of LLMs?
      </p>
      <p>
        CORAL is thus making important contributions to the future establishment of a German market for large language models.
      </p>
      
    </div>
    
    <div class="uk-container uk-margin-medium">
      <h2 id="events">Events</h2>
      {% include events.html %}
      
      <h2 id="publications">Publications</h2>
      <div>
        {% include publications.html %}
      </div>
      
      <h2 id="awards">Awards</h2>
      <div>
        {% include awards.html %}
      </div>
      
      <h2 id="partners">Partners</h2>
      
      <h3>Institute for Applied Informatics / Institute für Angewandte Informatik e.V.</h3>
      <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
        {% include people-cards/heyer.html %}
        {% include people-cards/schroeder.html %}
        {% include people-cards/gallagher.html %}
        {% include people-cards/stadler.html %}
        {% include people-cards/binder.html %}
      </div>
      
      <h3>University of Kassel / Universität Kassel</h3>
      <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
        {% include people-cards/potthast.html %}
        {% include people-cards/gienapp.html %}
        {% include people-cards/wiegmann.html %}
        {% include people-cards/ruth.html %}
      </div>
      
      <h3>Anhalt University of Applied Sciences / Hochschule Anhalt </h3>
      <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
        {% include people-cards/haenig.html %}
        {% include people-cards/hamotskyi.html %}
        {% include people-cards/gautam.html %}
      </div>
      
      <h3>German National Library / Deutsche Nationalbibliothek</h3>
      <div data-uk-grid class="uk-grid uk-grid-match uk-grid-small thumbnail-card-grid">
        {% include people-cards/leinen.html %}
        {% include people-cards/genet.html %}
        {% include people-cards/zimmermann.html %}
      </div>
      
      
    </div>
  </div>
</main>
